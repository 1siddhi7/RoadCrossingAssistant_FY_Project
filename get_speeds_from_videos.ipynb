{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitroadcrossconda9373e858eb374c278c9e03a6646bb46c",
   "display_name": "Python 3.7.3 64-bit ('roadcross': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/yagnesh/anaconda3/envs/roadcross/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import glob\n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import natsort\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import sys\n",
    "from random import randint\n",
    "from math import ceil, nan, sqrt\n",
    "import natsort\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import cv2\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerTypes = ['BOOSTING', 'MIL', 'KCF','TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "\n",
    "def createTrackerByName(trackerType):\n",
    "    \n",
    "  # Create a tracker based on tracker name\n",
    "    if trackerType == trackerTypes[0]:\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "  \n",
    "    elif trackerType == trackerTypes[1]: \n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "   \n",
    "    elif trackerType == trackerTypes[2]:\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "  \n",
    "    elif trackerType == trackerTypes[3]:\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "   \n",
    "    elif trackerType == trackerTypes[4]:\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    \n",
    "    elif trackerType == trackerTypes[5]:\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "            \n",
    "    elif trackerType == trackerTypes[6]:\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "        \n",
    "    elif trackerType == trackerTypes[7]:\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "        \n",
    "    else:\n",
    "        tracker = None\n",
    "        print('Incorrect tracker name')\n",
    "        print('Available trackers are:')\n",
    "        for t in trackerTypes:\n",
    "            print(t)\n",
    "     \n",
    "    return tracker\n",
    "\n",
    "\n",
    "def get_direction(videoPath, trackerType, boxes_list, frame_no):\n",
    "\n",
    "    '''\n",
    "    Detects direction of each vehicle in the video and returns and a list \n",
    "    containing these directions\n",
    "\n",
    "    Parameters:\n",
    "    videoPath(string) : path of the video\n",
    "    trackerType(string) : the openCV tracker type to be used  \n",
    "    boxes_list(list) : list containing bounding boxes for all detected vehicles \n",
    "    frame_no : frame no of the video for which you require directions\n",
    "\n",
    "    '''\n",
    "    k = 0 #iterator for frame no\n",
    "    multiTracker_back = cv2.MultiTracker_create()\n",
    "    #multiTracker = cv2.MultiTracker_create()\n",
    "    n = len(boxes_list)\n",
    "    directions = [0]*n\n",
    "    speed_array = [0]*n\n",
    "    x1 = [0]*n  #x coordinates of vehicles in frame_no\n",
    "    x2 = [0]*n  #x coordinates of vehicles in frame_no - 10\n",
    "\n",
    "    diff_x1 = [0]*n\n",
    "    diff_x2 = [0]*n\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX  # fontFace\n",
    "    fontScale = 1  # fontScale\n",
    "    fontColor = (255,255,255) # fontColor\n",
    "    lineType = 2 # lineType\n",
    "\n",
    "    #reading the video\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    cv2.namedWindow(videoPath, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(videoPath, height=760, width=1366)\n",
    "\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print('Failed to read video')\n",
    "        sys.exit(1)\n",
    "\n",
    "    center_point_x = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)/2)\n",
    "\n",
    "    while cap.isOpened() :\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        #saving past few frames as we will apply traking in backward direction\n",
    "        #form the frame_no\n",
    "        if( k == frame_no - 10):\n",
    "            frame_b10 = frame\n",
    "        if ( k == frame_no - 9):\n",
    "            frame_b9 = frame\n",
    "        if ( k == frame_no - 8):\n",
    "            frame_b8 = frame\n",
    "        if ( k == frame_no - 7):\n",
    "            frame_b7 = frame\n",
    "        if ( k == frame_no - 6):\n",
    "            frame_b6 = frame\n",
    "        if ( k == frame_no - 5):\n",
    "            frame_b5 = frame\n",
    "        if ( k == frame_no - 4):\n",
    "            frame_b4 = frame\n",
    "        if ( k == frame_no - 3):\n",
    "            frame_b3 = frame\n",
    "        if ( k == frame_no - 2):\n",
    "            frame_b2 = frame\n",
    "        if ( k == frame_no - 1):\n",
    "            frame_b1 = frame\n",
    "\n",
    "        if k == frame_no:\n",
    "            #boxes_list = [tuple(l) for l in boxes_list]\n",
    "            for i in range(n):\n",
    "                box = boxes_list[i]\n",
    "                x1[i] = (box[0]+2*box[2])/2\n",
    "                diff_x1[i] = x1[i] - center_point_x\n",
    "                multiTracker_back.add(createTrackerByName(trackerType), frame, box)\n",
    "            \n",
    "            #all the vehicles from boxes_list added in the traker instance\n",
    "            #applying vehicle tracking in backward direction\n",
    "                \n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b1)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b2)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b3)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b4)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b5)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b6)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b7)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b8)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b9)\n",
    "            (success_back, boxes_back) = multiTracker_back.update(frame_b10)\n",
    "\n",
    "\n",
    "            for i in range(n):\n",
    "                    b = boxes_back[i]\n",
    "                    x2[i] = (b[0]+2*b[2])/2\n",
    "                    diff_x2[i] = x2[i] - center_point_x\n",
    "\n",
    "\n",
    "            for i in range(n):\n",
    "                x_d =  diff_x2[i] - diff_x1[i]  # distance covered in x direction\n",
    "                x_distance = x2[i] - x1[i]\n",
    "                speed_array[i] = x_d / 10  # speed = distance / time\n",
    "                print(speed_array[i])\n",
    "                if x_distance > 0:\n",
    "                    directions[i] = 1\n",
    "                elif x_distance < 0:\n",
    "                    directions[i] = -1\n",
    "\n",
    "            for i in range(n):\n",
    "                box = boxes_list[i]\n",
    "                (x,y,w,h) = [int(v) for v in box]\n",
    "                if(directions[i] == 0): # not moving\n",
    "                    cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "                    cv2.putText(frame, str(speed_array[i]), org=(x,y-20), fontFace=font, fontScale=fontScale, color=fontColor, lineType=lineType)\n",
    "                elif(directions[i] == 1): # direction of interest\n",
    "                    cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "                    cv2.putText(frame, str(speed_array[i]), org=(x,y-20), fontFace=font, fontScale=fontScale, color=fontColor, lineType=lineType)\n",
    "                elif(directions[i] == -1): # opposite to direction of interest\n",
    "                    cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "\n",
    "            cv2.imshow(videoPath, frame)\n",
    "            cv2.waitKey(33)\n",
    "            # break\n",
    "        k = k+1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # return np.array(directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directions_from_videos(videos_folder, arrays_folder, trackerType = \"KCF\"):\n",
    "    arrays = glob.glob(arrays_folder+'/array*.npy')\n",
    "    arrays = natsort.natsorted(arrays)[101:]\n",
    "    videos = glob.glob(videos_folder+'/video*.MOV')\n",
    "    videos = natsort.natsorted(videos)[101:]\n",
    "\n",
    "    for (fname, vname) in zip(arrays, videos):\n",
    "        D = [[],[],[],[],[],[],[],[],[],[]] #since we won't be able to have past 10 frames for the first 10 frames\n",
    "        print(\"processing \",vname,\":\")\n",
    "        bounding_boxes = np.load(fname, allow_pickle=True) #loading the numpy array containing all detected vehicles\n",
    "        no_frames = bounding_boxes.shape[0]\n",
    "\n",
    "        for frame_no in range(10, no_frames):\n",
    "\n",
    "            temp = bounding_boxes[frame_no]\n",
    "            boxes_list = [(b[0],b[1],b[2]-b[0],b[3]-b[1]) for b in temp]\n",
    "\n",
    "            get_direction(vname, trackerType, boxes_list, frame_no)\n",
    "            # D.append(speed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing  /home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/videos/video102.MOV :\n",
      "-0.1\n",
      "-0.15\n",
      "-0.5\n",
      "-0.2\n",
      "-0.1\n",
      "-0.1\n",
      "6.6\n",
      "-0.15\n",
      "-0.15\n",
      "-0.5\n",
      "-0.15\n",
      "-0.15\n",
      "6.7\n",
      "-0.15\n",
      "-0.2\n",
      "-0.45\n",
      "-0.15\n",
      "-0.2\n",
      "-0.05\n",
      "6.85\n",
      "-0.1\n",
      "-0.05\n",
      "-0.45\n",
      "-2.95\n",
      "-0.2\n",
      "-0.05\n",
      "7.0\n",
      "0.0\n",
      "0.0\n",
      "-0.35\n",
      "-3.65\n",
      "-0.25\n",
      "7.1\n",
      "0.05\n",
      "-4.45\n",
      "0.05\n",
      "-4.55\n",
      "0.0\n",
      "0.05\n",
      "0.0\n",
      "-0.3\n",
      "0.1\n",
      "-5.25\n",
      "-5.25\n",
      "0.0\n",
      "0.0\n",
      "-0.3\n",
      "0.0\n",
      "0.05\n",
      "7.3\n",
      "-0.05\n",
      "-0.05\n",
      "-0.35\n",
      "-6.1\n",
      "-0.05\n",
      "-0.1\n",
      "-0.35\n",
      "7.3\n",
      "0.0\n",
      "-6.65\n",
      "-6.55\n",
      "-0.1\n",
      "-0.1\n",
      "-0.4\n",
      "0.0\n",
      "-0.1\n",
      "-0.55\n",
      "7.3\n",
      "-0.2\n",
      "-0.2\n",
      "-0.5\n",
      "-6.75\n",
      "-0.25\n",
      "-0.1\n",
      "-0.15\n",
      "-0.65\n",
      "7.25\n",
      "-0.3\n",
      "-0.3\n",
      "-0.6\n",
      "-0.25\n",
      "-0.75\n",
      "7.35\n",
      "-0.2\n",
      "-0.35\n",
      "-0.35\n",
      "-0.7\n",
      "-0.35\n",
      "7.25\n",
      "-0.2\n",
      "-0.5\n",
      "-0.45\n",
      "-0.85\n",
      "-0.5\n",
      "-0.45\n",
      "7.2\n",
      "-0.4\n",
      "-0.6\n",
      "-0.6\n",
      "-0.75\n",
      "-0.45\n",
      "-0.55\n",
      "-0.5\n",
      "7.2\n",
      "-0.5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ddb0a86304f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_directions_from_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/videos/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/arrays/arrays_v2_RetinaNet/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-1707a7d77ab2>\u001b[0m in \u001b[0;36mget_directions_from_videos\u001b[0;34m(videos_folder, arrays_folder, trackerType)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mboxes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mget_direction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# D.append(speed_array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-65ab4d6e9712>\u001b[0m in \u001b[0;36mget_direction\u001b[0;34m(videoPath, trackerType, boxes_list, frame_no)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msuccess_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_back\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiTracker_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_b1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0msuccess_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_back\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiTracker_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_b2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msuccess_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_back\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiTracker_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_b3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msuccess_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_back\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiTracker_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_b4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_directions_from_videos('/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/videos/', '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/arrays/arrays_v2_RetinaNet/')"
   ]
  }
 ]
}
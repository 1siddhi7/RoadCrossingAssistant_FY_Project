{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2.1:**\n",
    "\n",
    "**Training:**\n",
    "\n",
    "-> For every selected frame of video, extract features and give label(label generated by converting manually marked safe duration into frame intervals)\n",
    "\n",
    "-> Use classification to predict 0/1 for each frame\n",
    "\n",
    "**Testing :**\n",
    "\n",
    "For every frame, extract features of current frame, use previous k frames predict label as features and use classification to predict label\n",
    "\n",
    "**Features to be extracted from individual frames:**\n",
    "\n",
    "features extracted in Appraoch 1.2 + previous k(suppose k=10) number of frames predicted label using classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  \n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import sys\n",
    "from random import randint\n",
    "from math import ceil, sqrt\n",
    "import natsort\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import cv2\n",
    "from random import randint\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant path variables\n",
    "\n",
    "name = 'yagnesh'\n",
    "\n",
    "if name == 'siddhi':\n",
    "    path_train_arrays = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/arrays_train_v2'\n",
    "    path_test_arrays = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/arrays_test_v2'\n",
    "    path_train_labels = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/labels_train.csv'\n",
    "    path_test_labels = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/labels_test.csv'\n",
    "    path_direction_train_arrays = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/directions_train_v2'\n",
    "    path_direction_test_arrays = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/directions_test_v2'\n",
    "    path_train_videos = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/videos_train'\n",
    "    path_test_videos = '/home/siddhi/Desktop/RoadCrossingAssistant_FY_Project_Data/videos_test'\n",
    "\n",
    "\n",
    "elif name == 'yagnesh':\n",
    "    path_train_arrays = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/arrays_train_v2'   \n",
    "    path_test_arrays = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/arrays_test_v2'    \n",
    "    path_direction_train_arrays = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/directions_train_v2'   \n",
    "    path_direction_test_arrays = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/directions_test_v2'\n",
    "    path_train_labels = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_train.csv' \n",
    "    path_test_labels = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_test.csv'\n",
    "    loaded_predictions = np.load(\"/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/prediction_array_for_approach-2.1.npy\", allow_pickle=True)\n",
    "    path_saved_models = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_FY_Project/saved models'\n",
    "\n",
    "#version notes:\n",
    "#initial version : based on YOLOv3 model\n",
    "#v2 : based on RetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_region(x,y):\n",
    "\n",
    "    '''\n",
    "    Returns the region in which the particular point lies\n",
    "\n",
    "    Parameters:\n",
    "    x(int) : x coordinate of the point\n",
    "    y(int) : y coordinate of the point\n",
    "\n",
    "    Returns:\n",
    "    int : region no (1/2/3/4/5/6)\n",
    "    '''\n",
    "\n",
    "\n",
    "    video_height = 1080\n",
    "    video_width = 1920\n",
    "\n",
    "    if(x<640 and y>=540):\n",
    "        return 1 #Bottom left region\n",
    "    if(x<=1280 and y>=540):\n",
    "        return 2 #Bottom center region\n",
    "    if(x>1280 and y>=540):\n",
    "        return 3 #Bottom right region\n",
    "    if(x>1280 and y<540):\n",
    "        return 4 #Top right region\n",
    "    if(x<=1280 and y<540):\n",
    "        return 5 #Top center region\n",
    "    if(x<640 and y<540):\n",
    "        return 6 #Top left region\n",
    "\n",
    "def extract_features_from_frame(bounding_boxes, directions_for_boxes, frame_no):\n",
    "        \n",
    "    '''\n",
    "    Extract features from a particular frame\n",
    "\n",
    "    Parameters:\n",
    "    bounding_boxes(array) : detection array of the video\n",
    "    direction_for_boxes(array) : detection of direction for all bounding boxes \n",
    "    frame_no(int) : frame no for which features are to be extracted\n",
    "\n",
    "    Returns:\n",
    "    list : containing 18 features of current frame + predicted label of previous k frames as features of the frame\n",
    "    '''    \n",
    "\n",
    "    no_of_boxes = len(bounding_boxes[frame_no])  # total number of bounding boxes\n",
    "    boxes_frame = np.array(bounding_boxes[frame_no]) # bounding boxes for each frame \n",
    "    direction_frame = np.array(directions_for_boxes[frame_no]) # directions for every bounding boxes\n",
    "\n",
    "    # checking if no of bounding boxes and direction array length matches or not \n",
    "    if len(direction_frame) != no_of_boxes:\n",
    "        raise Exception('mismatch in number of boxes '+ str(len(direction_frame))+' '+str(no_of_boxes))\n",
    "\n",
    "    # matching indices of bounding boxes with required directions array\n",
    "    indices_required_boxes = np.where(direction_frame>=0)[0] # getting direction indices which is having value 0 or 1 (steady or positive direction vehicle)\n",
    "    boxes_required = boxes_frame[indices_required_boxes] # assigning indices of interested direction values (0 and 1 value of directions)\n",
    "\n",
    "    # feature - number of vehicles in each region\n",
    "    r1 = r2 = r3 = r4 = r5 = r6 = 0\n",
    "    \n",
    "    # feature - total area of vehicles in each region\n",
    "    r1_area = r2_area = r3_area = r4_area = r5_area = r6_area = 0\n",
    "\n",
    "    # distance calculation for every region\n",
    "    bottom_center_point_x = 1920/2    # width/2\n",
    "    bottom_center_point_y = 1080      # height\n",
    "    r1_min_distance = r2_min_distance = r3_min_distance = r4_min_distance = r5_min_distance = r6_min_distance = 0\n",
    "\n",
    "    for j in range(len(boxes_required)):\n",
    "\n",
    "        # center x coordinate of bounding box cx\n",
    "        cx = (boxes_required[j][0] + boxes_required[j][2])/2\n",
    "\n",
    "        # center y coordinate of bounding box cy\n",
    "        cy = (boxes_required[j][1] + boxes_required[j][3])/2\n",
    "\n",
    "        # area of bounding box\n",
    "        bounding_box_area = ((boxes_required[j][3]-boxes_required[j][1])*\n",
    "                        (boxes_required[j][2]-boxes_required[j][0]))\n",
    "\n",
    "        # distance calculation for every bounding box\n",
    "        distance = sqrt((cx - bottom_center_point_x)**2 + (cy - bottom_center_point_y)**2)\n",
    "\n",
    "        # finding region according to cx,cy and finding total area and minimum distance for each region\n",
    "        if(find_region(cx,cy)==1):\n",
    "            r1 = r1 + 1\n",
    "            r1_area = r1_area + bounding_box_area\n",
    "            if(r1_min_distance==0 or distance < r1_min_distance):\n",
    "                r1_min_distance = distance\n",
    "        elif(find_region(cx,cy)==2):\n",
    "            r2 = r2 + 1\n",
    "            r2_area = r2_area + bounding_box_area\n",
    "            if(r2_min_distance==0 or distance < r2_min_distance):\n",
    "                r2_min_distance = distance\n",
    "        elif(find_region(cx,cy)==3):\n",
    "            r3 = r3 + 1\n",
    "            r3_area = r3_area + bounding_box_area\n",
    "            if(r3_min_distance==0 or distance < r3_min_distance):\n",
    "                r3_min_distance = distance\n",
    "        elif(find_region(cx,cy)==4):\n",
    "            r4 = r4 + 1\n",
    "            r4_area = r4_area + bounding_box_area\n",
    "            if(r4_min_distance==0 or distance < r4_min_distance):\n",
    "                r4_min_distance = distance\n",
    "        elif(find_region(cx,cy)==5):\n",
    "            r5 = r5 + 1\n",
    "            r5_area = r5_area + bounding_box_area\n",
    "            if(r5_min_distance==0 or distance < r5_min_distance):\n",
    "                r5_min_distance = distance\n",
    "        elif(find_region(cx,cy)==6):\n",
    "            r6 = r6 + 1\n",
    "            r6_area = r6_area + bounding_box_area\n",
    "            if(r6_min_distance==0 or distance < r6_min_distance):\n",
    "                r6_min_distance = distance\n",
    "\n",
    "    # feature - predicted labels of previous k frames\n",
    "    # no_of_prev_frames = 10  # k = 10\n",
    "    # prev_frames_predictions = []\n",
    "\n",
    "    # if frame_no > no_of_prev_frames:\n",
    "    #     index = 0\n",
    "    #     current_frame_no = frame_no - 1\n",
    "    #     while index < no_of_prev_frames:\n",
    "    #         prev_frames_predictions.append(loaded_predictions[current_frame_no])\n",
    "    #         current_frame_no = current_frame_no - 1\n",
    "    #         index = index + 1\n",
    "\n",
    "        # features_frame = [r1,r1_area,r1_min_distance,r2,r2_area,r2_min_distance,r3,r3_area,r3_min_distance,r4,r4_area,r4_min_distance,r5,r5_area,r5_min_distance,r6,r6_area,r6_min_distance,prev_frames_predictions[0], prev_frames_predictions[1], prev_frames_predictions[2], prev_frames_predictions[3], prev_frames_predictions[4], prev_frames_predictions[5], prev_frames_predictions[6], prev_frames_predictions[7], prev_frames_predictions[8], prev_frames_predictions[9]]  # frame with total 28 frames (18 + k frames predicted label)\n",
    "    \n",
    "    # else:\n",
    "    #     features_frame = [-1]\n",
    "\n",
    "    features_frame = [r1,r1_area,r1_min_distance,r2,r2_area,r2_min_distance,r3,r3_area,r3_min_distance,r4,r4_area,r4_min_distance,r5,r5_area,r5_min_distance,r6,r6_area,r6_min_distance]  # 18 features per frame initially\n",
    "        \n",
    "    return features_frame  # final frame with 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_video(findex, no_frames, labels_csv):\n",
    "    \n",
    "    '''\n",
    "    Get labels for a particular video \n",
    "\n",
    "    Parameters:\n",
    "    findex(int) : video number in labels_csv for which labels are to be generated(starting from 0)\n",
    "    no_frames(int) : no of frames in the given video\n",
    "    labels_csv : the manually marked labels\n",
    "\n",
    "    Returns:\n",
    "    list : list with len = no of frames and the value at each index represents safe/unsafe at that frame_no (frame_no starting at 0)\n",
    "    int : -1 if there is no safe duration in video, 1 otherwise\n",
    "    '''\n",
    "\n",
    "    labels = [0]*no_frames\n",
    "    if(np.isnan(labels_csv[findex][0])):\n",
    "        return labels,-1 # there is no safe duration in the given video so all labels marked 0\n",
    "    else:\n",
    "        j = 0\n",
    "        while(j<len(labels_csv[findex]) and not(np.isnan(labels_csv[findex][j]))):\n",
    "            # mapping safe_start and safe_end in seconds to frame no\n",
    "            # frame number starts with 0\n",
    "            safe_start = max(int(labels_csv[findex][j]*30)-1,0) \n",
    "            safe_end = min(int(labels_csv[findex][j+1]*30)-1, no_frames-1)\n",
    "            labels[safe_start:safe_end+1] = [1]*(safe_end-safe_start+1) # marking the value b/w safe_start and safe_end with 1\n",
    "            j = j+2\n",
    "    if len(labels) > no_frames: # len of labels cannot be greater than no_frames in video\n",
    "        raise Exception('Check the labels assigned in CSV file!')\n",
    "    return labels,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate generate_datafrmae function with frames selected from unsafe videos\n",
    "def generate_dataframe(arrays_folder, direction_arrays_folder, findex, labels_csv):\n",
    "\n",
    "    '''\n",
    "    Generate dataframes for model training\n",
    "\n",
    "    Parameters:\n",
    "    arrays_folder(path) : folder containing detection arrays\n",
    "    direction_arrays_folder(path) : folder containing directions array\n",
    "    findex(int) : array number to start with from the given folder (starting from 0)\n",
    "    labels_csv : the manually marked labels\n",
    "\n",
    "    Returns:\n",
    "    features_dataframe : dataframe containg 18 features per frame\n",
    "    labels_dataframe : dataframe containing 1 label per frame\n",
    "    '''\n",
    "    \n",
    "    #getting all paths of all arrays in a list in proper order \n",
    "    arrays = glob.glob(arrays_folder+'/array*.npy')\n",
    "    arrays = natsort.natsorted(arrays)\n",
    "\n",
    "    # getting all paths of all direction arrays in a list in proper order\n",
    "    direction_arrays = glob.glob(direction_arrays_folder+'/directions*.npy')\n",
    "    direction_arrays = natsort.natsorted(direction_arrays)\n",
    "\n",
    "    cols_f = ['region1','region1_area','region1_min_distance','region2','region2_area','region2_min_distance','region3','region3_area','region3_min_distance','region4','region4_area','region4_min_distance','region5','region5_area','region5_min_distance','region6','region6_area','region6_min_distance','label_10','label_9','label_8','label_7','label_6','label_5','lable_4','label_3','label_2','label_1'] #18 + k features per frame\n",
    "    cols_l = ['safe/unsafe'] #1 label per frame\n",
    "    features_dataframe = pd.DataFrame()\n",
    "    labels_dataframe = pd.DataFrame()\n",
    "\n",
    "    for fname,directions_path in zip(arrays, direction_arrays):\n",
    "        \n",
    "        print(\"processing \",fname)\n",
    "        bounding_boxes = np.load(fname, allow_pickle=True) #loading the numpy array containing all detected vehicles\n",
    "        directions_for_boxes = np.load(directions_path, allow_pickle=True) # loading the arrays containing detected directions for each bounding box\n",
    "        no_frames = bounding_boxes.shape[0]\n",
    "        labels, flag = get_labels_from_video(findex, no_frames, labels_csv)\n",
    "\n",
    "        # labels_0 = [i for i, value in enumerate(labels) if value == 0 and i>6]\n",
    "\n",
    "        # if(flag == -1):   # no safe time\n",
    "        #     labels_1 = []\n",
    "        # else:\n",
    "        #     labels_1 = [i for i, value in enumerate(labels) if value == 1 and i>6]\n",
    "\n",
    "        features = []\n",
    "\n",
    "        # extracting features  framewise from labels\n",
    "        for frame_no in range(len(labels)):\n",
    "            features_frame = extract_features_from_frame(bounding_boxes, directions_for_boxes,frame_no)\n",
    "            features.append(features_frame)\n",
    "\n",
    "        previous_frames_labels_list = []  # queue for storing previous k = 10 frames predictions\n",
    "\n",
    "        #storing first 10 frames predictions using saved model of approach 1.2 from 18 features \n",
    "        for features_index in range(6,16):\n",
    "                previous_frames_labels_list[index] = approach_1_saved_model.predict(features[index])[0]\n",
    "        \n",
    "        # starting from frame no. 11,\n",
    "        # using queue, extending every features_frame with previous 10 frames predicted labels (for total 28 features per frame)\n",
    "        for i in range(10,len(features)):\n",
    "            current_frame_prediction = approach_1_saved_model.predict(features[i])[0]\n",
    "\n",
    "            # extending features using stored predictions in queue\n",
    "            features[i].extend(previous_frames_labels_list[:-1])\n",
    "\n",
    "            # delete first element and append current frame's prediction at the end of the queue\n",
    "            del previous_frames_labels_list[0]\n",
    "            previous_frames_labels_list.append(current_frame_prediction)\n",
    "\n",
    "        features = features[10:]  # ignoring first 10 frames for dataframe\n",
    "        labels = labels[10:]\n",
    "        if len(features) != len(labels):\n",
    "            raise Exception(\"features and labels are not of same lengths\")\n",
    "\n",
    "        # filtering and selecting feature frames\n",
    "        labels_0_indices = np.where(labels == 0)[0]  # getting indices of unsafe labels\n",
    "        if(len(labels_0_indices) >= 50):\n",
    "            labels_0_indices = labels_0_indices[:50]\n",
    "\n",
    "        if flag == -1:  # no safe duration in video\n",
    "            labels_1_indices = []  # safe labels will be empty\n",
    "        else:\n",
    "            labels_1_indices = np.where(labels == 1)[0]  # getting indices of safe labels\n",
    "            if len(labels_0_indices) >= len(labels_1_indices):\n",
    "                labels_0_indices = labels_0_indices[:len(labels_1_indices)]\n",
    "\n",
    "        labels_indices = np.append(labels_0_indices, labels_1_indices)  # label frame indices\n",
    "\n",
    "        labels = labels[labels_indices]  # final labels for video\n",
    "        features = features[labels_indices] # final features for video\n",
    "\n",
    "        findex = findex + 1\n",
    "        df1 = pd.DataFrame(features, columns=cols_f)\n",
    "        features_dataframe = features_dataframe.append(df1,ignore_index=True)\n",
    "        #print(\"df1.size\",df1.shape)\n",
    "        df2 = pd.DataFrame(labels, columns=cols_l)\n",
    "        labels_dataframe = labels_dataframe.append(df2,ignore_index=True)\n",
    "        #print(\"df2.size\",df2.shape)\n",
    "        \n",
    "    return features_dataframe, labels_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60, 4) (16, 6)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "labels_train = genfromtxt(path_train_labels, delimiter =',')\n",
    "labels_test = genfromtxt(path_test_labels, delimiter =',')\n",
    "print(labels_train.shape, labels_test.shape)"
   ]
  },
  {
   "source": [
    "## Generating Dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing  /home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/arrays_train_v2/array1.npy\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "mismatch in number of boxes 0 8",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-823f8280c05d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mapproach_1_saved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures_df_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_direction_train_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# features_df_train.to_pickle('features_df_train.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# labels_df_train.to_pickle('labels_df_train.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-2aea266f75b8>\u001b[0m in \u001b[0;36mgenerate_dataframe\u001b[0;34m(arrays_folder, direction_arrays_folder, findex, labels_csv)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# extracting features  framewise from labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mfeatures_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_from_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_for_boxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-4736cf0a7cac>\u001b[0m in \u001b[0;36mextract_features_from_frame\u001b[0;34m(bounding_boxes, directions_for_boxes, frame_no)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# checking if no of bounding boxes and direction array length matches or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection_frame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mno_of_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mismatch in number of boxes '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# matching indices of bounding boxes with required directions array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: mismatch in number of boxes 0 8"
     ]
    }
   ],
   "source": [
    "saved_model_file = path_saved_models + '/approach-1.2-classifier.sav'\n",
    "approach_1_saved_model = pickle.load(open(saved_model_file,'rb'))\n",
    "\n",
    "features_df_train, labels_df_train = generate_dataframe(path_train_arrays, path_direction_train_arrays,0,labels_train)\n",
    "# features_df_train.to_pickle('features_df_train.pkl')\n",
    "# labels_df_train.to_pickle('labels_df_train.pkl')\n",
    "\n",
    "features_df_test, labels_df_test = generate_dataframe(path_test_arrays, path_direction_test_arrays, 0,labels_test)\n",
    "# features_df_test.to_pickle('features_df_test.pkl')\n",
    "# labels_df_test.to_pickle('labels_df_test.pkl')\n",
    "print(\"features_df_train.shape: \", features_df_train.shape)\n",
    "print(\"features_df_test.shape: \", features_df_test.shape)\n",
    "print(\"labels_df_train.shape: \", labels_df_train.shape)\n",
    "print(\"labels_df_test.shape: \", labels_df_test.shape)"
   ]
  },
  {
   "source": [
    "## Feature scaling\n",
    "Strategies that can be used: \n",
    "### 1. MinMaxScaler\n",
    "### 2. StandardScaler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# StandardScaler\n",
    "#feature_scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler\n",
    "feature_scaler = MinMaxScaler()\n",
    "\n",
    "# storing scaled features\n",
    "features_train_scaled = pd.DataFrame(feature_scaler.fit_transform(features_df_train))\n",
    "features_train_scaled.columns = ['region1','region1_area','region1_min_distance','region2','region2_area','region2_min_distance','region3','region3_area','region3_min_distance','region4','region4_area','region4_min_distance','region5','region5_area','region5_min_distance','region6','region6_area','region6_min_distance','label_10','label_9','label_8','label_7','label_6','label_5','lable_4','label_3','label_2','label_1']\n",
    "features_test_scaled = pd.DataFrame(feature_scaler.transform(features_df_test))\n",
    "features_test_scaled.columns = ['region1','region1_area','region1_min_distance','region2','region2_area','region2_min_distance','region3','region3_area','region3_min_distance','region4','region4_area','region4_min_distance','region5','region5_area','region5_min_distance','region6','region6_area','region6_min_distance','label_10','label_9','label_8','label_7','label_6','label_5','lable_4','label_3','label_2','label_1']"
   ]
  },
  {
   "source": [
    "## Model Training\n",
    "### Classification model: Support Vector Machine with rbf kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "for train data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85      7384\n",
      "           1       0.70      0.43      0.53      3236\n",
      "\n",
      "    accuracy                           0.77     10620\n",
      "   macro avg       0.74      0.68      0.69     10620\n",
      "weighted avg       0.76      0.77      0.75     10620\n",
      "\n",
      "accuracy score 0.77015065913371\n",
      "\n",
      "for test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.74      1541\n",
      "           1       0.74      0.26      0.38      1223\n",
      "\n",
      "    accuracy                           0.63      2764\n",
      "   macro avg       0.67      0.59      0.56      2764\n",
      "weighted avg       0.67      0.63      0.58      2764\n",
      "\n",
      "accuracy score 0.6313314037626628\n"
     ]
    }
   ],
   "source": [
    "# scaled dataset\n",
    "model = SVC()\n",
    "model.fit(features_train_scaled, labels_df_train) \n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "# print prediction results \n",
    "print(\"for train data:\")\n",
    "predictions = model.predict(features_train_scaled) \n",
    "print(classification_report(labels_df_train, predictions)) \n",
    "print(\"accuracy score\", accuracy_score(labels_df_train, predictions))\n",
    "\n",
    "print(\"\\nfor test data:\")\n",
    "predictions = model.predict(features_test_scaled) \n",
    "print(classification_report(labels_df_test, predictions)) \n",
    "print(\"accuracy score\",accuracy_score(labels_df_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "#checking predictions for individual frames\n",
    "\n",
    "def prediction_for_frame(aname, dname, frame_no):\n",
    "\n",
    "    bounding_boxes = np.load(aname, allow_pickle=True)\n",
    "    dirs = np.load(dname, allow_pickle=True)\n",
    "\n",
    "    features = extract_features_from_frame(bounding_boxes, dirs, frame_no)\n",
    "\n",
    "    f = []\n",
    "    f.append(features)\n",
    "    cols_f = ['region1','region1_area','region1_min_distance','region2','region2_area','region2_min_distance','region3','region3_area','region3_min_distance','region4','region4_area','region4_min_distance','region5','region5_area','region5_min_distance','region6','region6_area','region6_min_distance','label_10','label_9','label_8','label_7','label_6','label_5','lable_4','label_3','label_2','label_1']\n",
    "    f_df = pd.DataFrame(f, columns=cols_f)\n",
    "\n",
    "    f_df_scaled = pd.DataFrame(feature_scaler.transform(f_df))\n",
    "    f_df_scaled.columns = cols_f\n",
    "\n",
    "    print(\"prediction: \",model.predict(f_df_scaled))\n",
    "\n",
    "def show_frame(vname, frame_no): \n",
    "      \n",
    "    # Path to video file \n",
    "    vidObj = cv2.VideoCapture(vname) \n",
    "    # Used as counter variable \n",
    "    count = 0\n",
    "    # checks whether frames were extracted \n",
    "    success = 1\n",
    "  \n",
    "    while success: \n",
    "  \n",
    "        # vidObj object calls read \n",
    "        # function extract frames \n",
    "        success, image = vidObj.read() \n",
    "        if(count == frame_no):\n",
    "        # Saves the frames with frame-count \n",
    "            cv2.imwrite(\"frame%d.jpg\" % count, image)\n",
    "            break \n",
    "  \n",
    "        count += 1\n",
    "\n",
    "\n",
    "aname = path_test_arrays + \"/array62.npy\"\n",
    "vname = \"/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/video62.MOV\"\n",
    "dname = path_direction_test_arrays + \"/directions62.npy\"\n",
    "frame_no  = 160\n",
    "\n",
    "prediction_for_frame(aname, dname, frame_no)\n",
    "show_frame(vname, frame_no)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37364bitroadcrossconda9373e858eb374c278c9e03a6646bb46c",
   "display_name": "Python 3.7.3 64-bit ('roadcross': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
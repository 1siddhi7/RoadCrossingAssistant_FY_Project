{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda3/envs/roadcross/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import sys\n",
    "from random import randint\n",
    "from math import ceil, sqrt\n",
    "import natsort\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'aws'\n",
    "\n",
    "if user == 'siddhi':\n",
    "    path_videos = 'C:/RoadCrossingAssistant/Data/Videos/'\n",
    "    path_labels_csv = 'C:/RoadCrossingAssistant/Data/labels_framewise_csv.csv'\n",
    "    path_labels_list = 'C:/RoadCrossingAssistant/Data/labels_framewise_list.pkl'\n",
    "\n",
    "elif user == 'yagnesh':\n",
    "    path_videos = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/videos/'\n",
    "    path_labels_csv = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_framewise.csv'\n",
    "    path_labels_list = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_framewise.pkl'\n",
    "\n",
    "elif user == 'aws':\n",
    "    path_videos = '/data/Data/Videos/'\n",
    "    path_labels_csv = '/data/Data/labels_framewise_csv.csv'\n",
    "    path_labels_list = '/data/Data/labels_framewise_list.pkl'\n",
    "\n",
    "videos = glob.glob(path_videos+'video*.MOV')\n",
    "videos = natsort.natsorted(videos)\n",
    "\n",
    "# frame-wise labels array\n",
    "open_file = open(path_labels_list, \"rb\")\n",
    "labels_list = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of videos_train:  80\n",
      "len of videos_test:  24\n",
      "len of labels_train_loaded:  80\n",
      "len of labels_test_loaded:  24\n"
     ]
    }
   ],
   "source": [
    "#Perform train-test split(80-24)\n",
    "\n",
    "x = np.arange(104)\n",
    "#np.random.seed(42)\n",
    "indices_test = np.random.choice(x, 24, replace=False)\n",
    "indices_train = np.delete(x, indices_test, axis=0)\n",
    "\n",
    "videos_train = [videos[ind] for ind in indices_train]\n",
    "videos_test = [videos[ind] for ind in indices_test]\n",
    "\n",
    "labels_train_loaded = [labels_list[ind] for ind in indices_train]\n",
    "labels_test_loaded = [labels_list[ind] for ind in indices_test]\n",
    "\n",
    "print('len of videos_train: ', len(videos_train))\n",
    "print('len of videos_test: ', len(videos_test))\n",
    "print('len of labels_train_loaded: ', len(labels_train_loaded))\n",
    "print('len of labels_test_loaded: ', len(labels_test_loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_video(no_frames, safe_duration_list):\n",
    "    \n",
    "    '''\n",
    "    Get labels for a particular video \n",
    "\n",
    "    Parameters:\n",
    "    no_frames(int) : no of frames in the given video\n",
    "    safe_duration_list(list) : a list of the type [safe_start1, safe_end1, safe_start2, safe_end2,......]\n",
    "\n",
    "    Returns:\n",
    "    list : list with len = no of frames and the value at each index represents safe/unsafe at that frame_no (frame_no starting at 0)\n",
    "    int : -1 if there is no safe duration in video, 1 otherwise\n",
    "    '''\n",
    "\n",
    "    labels = [0]*no_frames\n",
    "    no_safe_durations = int(len(safe_duration_list)/2)\n",
    "    if(no_safe_durations == 0):\n",
    "        return labels,-1 # there is no safe duration in the given video so all labels marked 0\n",
    "    else:\n",
    "\n",
    "        for i in range(no_safe_durations):\n",
    "            safe_start = max(safe_duration_list[i*2] - 1, 0)\n",
    "            safe_end = min(safe_duration_list[i*2 +1] - 1, no_frames-1)\n",
    "            labels[safe_start:safe_end+1] = [1]*(safe_end-safe_start+1) # marking the value b/w safe_start and safe_end with 1\n",
    "\n",
    "    if len(labels) > no_frames: #len of labels cannot be greater than no_frames in video\n",
    "        raise Exception('Check the labels assigned in CSV file!')\n",
    "    return labels,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Data/Videos/video1.MOV\n",
      "/data/Data/Videos/video2.MOV\n",
      "/data/Data/Videos/video3.MOV\n",
      "/data/Data/Videos/video5.MOV\n",
      "/data/Data/Videos/video6.MOV\n",
      "/data/Data/Videos/video7.MOV\n",
      "/data/Data/Videos/video8.MOV\n",
      "/data/Data/Videos/video9.MOV\n",
      "/data/Data/Videos/video11.MOV\n",
      "/data/Data/Videos/video12.MOV\n",
      "/data/Data/Videos/video13.MOV\n",
      "/data/Data/Videos/video14.MOV\n",
      "/data/Data/Videos/video17.MOV\n",
      "/data/Data/Videos/video18.MOV\n",
      "/data/Data/Videos/video19.MOV\n",
      "/data/Data/Videos/video20.MOV\n",
      "/data/Data/Videos/video24.MOV\n",
      "/data/Data/Videos/video25.MOV\n",
      "/data/Data/Videos/video26.MOV\n",
      "/data/Data/Videos/video27.MOV\n",
      "/data/Data/Videos/video28.MOV\n",
      "/data/Data/Videos/video29.MOV\n",
      "/data/Data/Videos/video30.MOV\n",
      "/data/Data/Videos/video31.MOV\n",
      "/data/Data/Videos/video34.MOV\n",
      "/data/Data/Videos/video35.MOV\n",
      "/data/Data/Videos/video36.MOV\n",
      "/data/Data/Videos/video37.MOV\n",
      "/data/Data/Videos/video38.MOV\n",
      "/data/Data/Videos/video39.MOV\n",
      "/data/Data/Videos/video40.MOV\n",
      "/data/Data/Videos/video41.MOV\n",
      "/data/Data/Videos/video43.MOV\n",
      "/data/Data/Videos/video44.MOV\n",
      "/data/Data/Videos/video45.MOV\n",
      "/data/Data/Videos/video47.MOV\n",
      "/data/Data/Videos/video48.MOV\n",
      "/data/Data/Videos/video49.MOV\n",
      "/data/Data/Videos/video50.MOV\n",
      "/data/Data/Videos/video51.MOV\n",
      "/data/Data/Videos/video52.MOV\n",
      "/data/Data/Videos/video53.MOV\n",
      "/data/Data/Videos/video54.MOV\n",
      "/data/Data/Videos/video55.MOV\n",
      "/data/Data/Videos/video57.MOV\n",
      "/data/Data/Videos/video58.MOV\n",
      "/data/Data/Videos/video61.MOV\n",
      "/data/Data/Videos/video62.MOV\n",
      "/data/Data/Videos/video63.MOV\n",
      "/data/Data/Videos/video64.MOV\n",
      "/data/Data/Videos/video65.MOV\n",
      "/data/Data/Videos/video66.MOV\n",
      "/data/Data/Videos/video67.MOV\n",
      "/data/Data/Videos/video68.MOV\n",
      "/data/Data/Videos/video70.MOV\n",
      "/data/Data/Videos/video71.MOV\n",
      "/data/Data/Videos/video72.MOV\n",
      "/data/Data/Videos/video74.MOV\n",
      "/data/Data/Videos/video75.MOV\n",
      "/data/Data/Videos/video76.MOV\n",
      "/data/Data/Videos/video77.MOV\n",
      "/data/Data/Videos/video80.MOV\n",
      "/data/Data/Videos/video81.MOV\n",
      "/data/Data/Videos/video82.MOV\n",
      "/data/Data/Videos/video84.MOV\n",
      "/data/Data/Videos/video85.MOV\n",
      "/data/Data/Videos/video86.MOV\n",
      "/data/Data/Videos/video87.MOV\n",
      "/data/Data/Videos/video91.MOV\n",
      "/data/Data/Videos/video92.MOV\n",
      "/data/Data/Videos/video94.MOV\n",
      "/data/Data/Videos/video95.MOV\n",
      "/data/Data/Videos/video96.MOV\n",
      "/data/Data/Videos/video97.MOV\n",
      "/data/Data/Videos/video98.MOV\n",
      "/data/Data/Videos/video99.MOV\n",
      "/data/Data/Videos/video100.MOV\n",
      "/data/Data/Videos/video102.MOV\n",
      "/data/Data/Videos/video103.MOV\n",
      "/data/Data/Videos/video104.MOV\n",
      "/data/Data/Videos/video4.MOV\n",
      "/data/Data/Videos/video22.MOV\n",
      "/data/Data/Videos/video101.MOV\n",
      "/data/Data/Videos/video23.MOV\n",
      "/data/Data/Videos/video88.MOV\n",
      "/data/Data/Videos/video15.MOV\n",
      "/data/Data/Videos/video10.MOV\n",
      "/data/Data/Videos/video90.MOV\n",
      "/data/Data/Videos/video79.MOV\n",
      "/data/Data/Videos/video73.MOV\n",
      "/data/Data/Videos/video59.MOV\n",
      "/data/Data/Videos/video16.MOV\n",
      "/data/Data/Videos/video93.MOV\n",
      "/data/Data/Videos/video42.MOV\n",
      "/data/Data/Videos/video89.MOV\n",
      "/data/Data/Videos/video60.MOV\n",
      "/data/Data/Videos/video69.MOV\n",
      "/data/Data/Videos/video56.MOV\n",
      "/data/Data/Videos/video32.MOV\n",
      "/data/Data/Videos/video21.MOV\n",
      "/data/Data/Videos/video33.MOV\n",
      "/data/Data/Videos/video46.MOV\n",
      "/data/Data/Videos/video78.MOV\n",
      "/data/Data/Videos/video83.MOV\n",
      "shape of labels_train (20220,)\n",
      "shape of labels_test (6300,)\n"
     ]
    }
   ],
   "source": [
    "labels_train = []\n",
    "labels_test = []\n",
    "\n",
    "for i in range(len(videos_train)):\n",
    "\n",
    "    print(videos_train[i])\n",
    "    cap = cv2.VideoCapture(videos_train[i])\n",
    "    no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    l, f = get_labels_from_video(no_frames, labels_train_loaded[i])\n",
    "    labels_train.extend(l)\n",
    "\n",
    "for i in range(len(videos_test)):\n",
    "\n",
    "    print(videos_test[i])\n",
    "    cap = cv2.VideoCapture(videos_test[i])\n",
    "    no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    l, f = get_labels_from_video(no_frames, labels_test_loaded[i])\n",
    "    labels_test.extend(l)\n",
    "\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(\"shape of labels_train\", labels_train.shape)\n",
    "print(\"shape of labels_test\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13433,) (6787,)\n",
      "reducing the number of unsafe frames\n",
      "\n",
      "\n",
      "(4004,) (2296,)\n",
      "reducing the number of unsafe frames\n",
      "\n",
      "\n",
      "shape on labels_train_reduced (14931,)\n",
      "shape of labels_test_reduced (5051,)\n"
     ]
    }
   ],
   "source": [
    "#get required frame indices\n",
    "\n",
    "def get_required_indices(labels):\n",
    "\n",
    "    ind0 = np.where(labels == 0)[0]\n",
    "    ind1 = np.where(labels == 1)[0]\n",
    "\n",
    "    print(ind0.shape, ind1.shape)\n",
    "\n",
    "    if (len(ind0)/len(ind1)) > 1.2:\n",
    "        print('reducing the number of unsafe frames\\n\\n')\n",
    "        len_ind0 = int(len(ind1)*1.2)\n",
    "        ind0 = ind0[:len_ind0]\n",
    "\n",
    "        indices_required = np.concatenate((ind0, ind1))\n",
    "    \n",
    "    return indices_required\n",
    "\n",
    "indices_required_train = get_required_indices(labels_train)\n",
    "indices_required_test = get_required_indices(labels_test)\n",
    "\n",
    "labels_train_reduced = labels_train[indices_required_train]\n",
    "labels_test_reduced = labels_test[indices_required_test]\n",
    "\n",
    "print(\"shape on labels_train_reduced\", labels_train_reduced.shape)\n",
    "print(\"shape of labels_test_reduced\", labels_test_reduced.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Data/Videos/video1.MOV\n",
      "/data/Data/Videos/video2.MOV\n",
      "/data/Data/Videos/video3.MOV\n",
      "/data/Data/Videos/video5.MOV\n",
      "/data/Data/Videos/video6.MOV\n",
      "/data/Data/Videos/video7.MOV\n",
      "/data/Data/Videos/video8.MOV\n",
      "/data/Data/Videos/video9.MOV\n",
      "/data/Data/Videos/video11.MOV\n",
      "/data/Data/Videos/video12.MOV\n",
      "/data/Data/Videos/video13.MOV\n",
      "/data/Data/Videos/video14.MOV\n",
      "/data/Data/Videos/video17.MOV\n",
      "/data/Data/Videos/video18.MOV\n",
      "/data/Data/Videos/video19.MOV\n",
      "/data/Data/Videos/video20.MOV\n",
      "/data/Data/Videos/video24.MOV\n",
      "/data/Data/Videos/video25.MOV\n",
      "/data/Data/Videos/video26.MOV\n",
      "/data/Data/Videos/video27.MOV\n",
      "/data/Data/Videos/video28.MOV\n",
      "/data/Data/Videos/video29.MOV\n",
      "/data/Data/Videos/video30.MOV\n",
      "/data/Data/Videos/video31.MOV\n",
      "/data/Data/Videos/video34.MOV\n",
      "/data/Data/Videos/video35.MOV\n",
      "/data/Data/Videos/video36.MOV\n",
      "/data/Data/Videos/video37.MOV\n",
      "/data/Data/Videos/video38.MOV\n",
      "/data/Data/Videos/video39.MOV\n",
      "/data/Data/Videos/video40.MOV\n",
      "/data/Data/Videos/video41.MOV\n",
      "/data/Data/Videos/video43.MOV\n",
      "/data/Data/Videos/video44.MOV\n",
      "/data/Data/Videos/video45.MOV\n",
      "/data/Data/Videos/video47.MOV\n",
      "/data/Data/Videos/video48.MOV\n",
      "/data/Data/Videos/video49.MOV\n",
      "/data/Data/Videos/video50.MOV\n",
      "/data/Data/Videos/video51.MOV\n",
      "/data/Data/Videos/video52.MOV\n",
      "/data/Data/Videos/video53.MOV\n",
      "/data/Data/Videos/video54.MOV\n",
      "/data/Data/Videos/video55.MOV\n",
      "/data/Data/Videos/video57.MOV\n",
      "/data/Data/Videos/video58.MOV\n",
      "/data/Data/Videos/video61.MOV\n",
      "/data/Data/Videos/video62.MOV\n",
      "/data/Data/Videos/video63.MOV\n",
      "/data/Data/Videos/video64.MOV\n",
      "/data/Data/Videos/video65.MOV\n",
      "/data/Data/Videos/video66.MOV\n",
      "/data/Data/Videos/video67.MOV\n",
      "/data/Data/Videos/video68.MOV\n",
      "/data/Data/Videos/video70.MOV\n",
      "/data/Data/Videos/video71.MOV\n",
      "/data/Data/Videos/video72.MOV\n",
      "/data/Data/Videos/video74.MOV\n",
      "/data/Data/Videos/video75.MOV\n",
      "/data/Data/Videos/video76.MOV\n",
      "/data/Data/Videos/video77.MOV\n",
      "/data/Data/Videos/video80.MOV\n",
      "/data/Data/Videos/video81.MOV\n",
      "/data/Data/Videos/video82.MOV\n",
      "/data/Data/Videos/video84.MOV\n",
      "/data/Data/Videos/video85.MOV\n",
      "/data/Data/Videos/video86.MOV\n",
      "/data/Data/Videos/video87.MOV\n",
      "/data/Data/Videos/video91.MOV\n",
      "/data/Data/Videos/video92.MOV\n",
      "/data/Data/Videos/video94.MOV\n",
      "/data/Data/Videos/video95.MOV\n",
      "/data/Data/Videos/video96.MOV\n",
      "/data/Data/Videos/video97.MOV\n",
      "/data/Data/Videos/video98.MOV\n",
      "/data/Data/Videos/video99.MOV\n",
      "/data/Data/Videos/video100.MOV\n",
      "/data/Data/Videos/video102.MOV\n",
      "/data/Data/Videos/video103.MOV\n",
      "/data/Data/Videos/video104.MOV\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_required_frames(videos, indices_required):\n",
    "\n",
    "    frames = []\n",
    "    ind = -1\n",
    "\n",
    "    for i in range(len(videos)):\n",
    "\n",
    "        print(videos[i])\n",
    "        cap = cv2.VideoCapture(videos[i])\n",
    "        no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        #print(no_frames)\n",
    "        \n",
    "        while cap.isOpened() :\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            ind = ind + 1\n",
    "            if ind in indices_required:\n",
    "                #frames_train.append(frame)\n",
    "                #print(frame.shape)\n",
    "                frame_resized = cv2.resize(frame, (360, 640), interpolation = cv2.INTER_AREA)\n",
    "                #print(frame_resized.shape)\n",
    "                frames.append(frame_resized)\n",
    "    return np.array(frames)\n",
    "\n",
    "frames_train = get_required_frames(videos_train, indices_required_train)\n",
    "frames_test = get_required_frames(videos_test, indices_required_test)\n",
    "\n",
    "print(\"shape of frames_train: \",frames_train.shape)\n",
    "print(\"shape of frames_test: \",frames_test.shape)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#print(labels_train_reduced)\n",
    "y_train = to_categorical(labels_train_reduced, 2)\n",
    "y_test = to_categorical(labels_test_reduced, 2)\n",
    "#print(y_train)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255, brightness_range=[0.4, 1.5], channel_shift_range = 50, validation_split=0.2)\n",
    "\n",
    "# train_generator = datagen.flow(frames_train,\n",
    "#                                y_train,\n",
    "#                                batch_size=16)\n",
    "\n",
    "# datagen = ImageDataGenerator(rescale=1./255.,validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow(frames_train,\n",
    "                               y_train,\n",
    "                               batch_size=16,\n",
    "                               shuffle=True,\n",
    "                               subset='training')\n",
    "\n",
    "valid_generator = datagen.flow(frames_train,\n",
    "                               y_train,\n",
    "                               batch_size=16,\n",
    "                               subset='validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import imread, imshow, subplots, show\n",
    "# def plot(data_generator):\n",
    "#     \"\"\"\n",
    "#     Plots 4 images generated by an object of the ImageDataGenerator class.\n",
    "#     \"\"\"\n",
    "#     data_generator.fit(images)\n",
    "#     image_iterator = data_generator.flow(images)\n",
    "    \n",
    "#     # Plot the images given by the iterator\n",
    "#     fig, rows = subplots(nrows=1, ncols=4, figsize=(18,18))\n",
    "#     for row in rows:\n",
    "#         row.imshow(image_iterator.next()[0].astype('int'))\n",
    "#         row.axis('off')\n",
    "#     show()\n",
    "\n",
    "# image = imread(\"C:/RoadCrossingAssistant/Website/assets/img/approach_1.1_frames/fn1.jpg\")\n",
    "\n",
    "# # Creating a dataset which contains just one image.\n",
    "# images = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# #imshow(images[0])\n",
    "# #show()\n",
    "\n",
    "# data_generator = ImageDataGenerator(channel_shift_range = 50)\n",
    "# plot(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.applications import MobileNet\n",
    "\n",
    "base_model = MobileNet(input_shape = (640, 360, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = tensorflow.keras.models.Model(base_model.input, x)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, shuffle='true', epochs=18, verbose=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(frames_test, y_test, batch_size=16)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(\"Evaluate on train data\")\n",
    "results = model.evaluate(frames_train, y_train, batch_size=16)\n",
    "print(\"train loss, t acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions\")\n",
    "predictions = model.predict(frames_test)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(np.argmax(y_test, axis = 1), np.argmax(predictions, axis = 1))\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([0,0,1,1,1])\n",
    "# vv = to_categorical(x)\n",
    "# print(vv)\n",
    "# np.argmax(a, axis = 1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

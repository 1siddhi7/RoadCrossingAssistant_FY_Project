{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('roadcross': conda)",
   "metadata": {
    "interpreter": {
     "hash": "efbaad368bcd49cf5b8b1d4bc08527738fe3c05d47d6e3ea110540aa6c36949d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  \n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import sys\n",
    "from random import randint\n",
    "from math import ceil, sqrt\n",
    "import natsort\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'siddhi'\n",
    "\n",
    "if user == 'siddhi':\n",
    "    path_videos = 'C:/RoadCrossingAssistant/Data/Videos/'\n",
    "    path_labels_csv = 'C:/RoadCrossingAssistant/Data/labels_framewise_csv.csv'\n",
    "    path_labels_list = 'C:/RoadCrossingAssistant/Data/labels_framewise_list.pkl'\n",
    "\n",
    "elif user == 'yagnesh':\n",
    "    path_videos = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/videos/'\n",
    "    path_labels_csv = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_framewise.csv'\n",
    "    path_labels_list = '/home/yagnesh/Study/Machine Learning/ML projects/RoadCrossingAssistant_Arrays/labels_framewise.pkl'\n",
    "\n",
    "videos = glob.glob(path_videos+'video*.MOV')\n",
    "videos = natsort.natsorted(videos)\n",
    "\n",
    "# frame-wise labels array\n",
    "open_file = open(path_labels_list, \"rb\")\n",
    "labels_list = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len of videos_train:  80\nlen of videos_test:  24\nlen of labels_train_loaded:  80\nlen of labels_test_loaded:  24\n"
     ]
    }
   ],
   "source": [
    "#Perform train-test split(80-24)\n",
    "\n",
    "x = np.arange(104)\n",
    "#np.random.seed(42)\n",
    "indices_test = np.random.choice(x, 24, replace=False)\n",
    "indices_train = np.delete(x, indices_test, axis=0)\n",
    "\n",
    "videos_train = [videos[ind] for ind in indices_train]\n",
    "videos_test = [videos[ind] for ind in indices_test]\n",
    "\n",
    "labels_train_loaded = [labels_list[ind] for ind in indices_train]\n",
    "labels_test_loaded = [labels_list[ind] for ind in indices_test]\n",
    "\n",
    "print('len of videos_train: ', len(videos_train))\n",
    "print('len of videos_test: ', len(videos_test))\n",
    "print('len of labels_train_loaded: ', len(labels_train_loaded))\n",
    "print('len of labels_test_loaded: ', len(labels_test_loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_video(no_frames, safe_duration_list):\n",
    "    \n",
    "    '''\n",
    "    Get labels for a particular video \n",
    "\n",
    "    Parameters:\n",
    "    no_frames(int) : no of frames in the given video\n",
    "    safe_duration_list(list) : a list of the type [safe_start1, safe_end1, safe_start2, safe_end2,......]\n",
    "\n",
    "    Returns:\n",
    "    list : list with len = no of frames and the value at each index represents safe/unsafe at that frame_no (frame_no starting at 0)\n",
    "    int : -1 if there is no safe duration in video, 1 otherwise\n",
    "    '''\n",
    "\n",
    "    labels = [0]*no_frames\n",
    "    no_safe_durations = int(len(safe_duration_list)/2)\n",
    "    if(no_safe_durations == 0):\n",
    "        return labels,-1 # there is no safe duration in the given video so all labels marked 0\n",
    "    else:\n",
    "\n",
    "        for i in range(no_safe_durations):\n",
    "            safe_start = max(safe_duration_list[i*2] - 1, 0)\n",
    "            safe_end = min(safe_duration_list[i*2 +1] - 1, no_frames-1)\n",
    "            labels[safe_start:safe_end+1] = [1]*(safe_end-safe_start+1) # marking the value b/w safe_start and safe_end with 1\n",
    "\n",
    "    if len(labels) > no_frames: #len of labels cannot be greater than no_frames in video\n",
    "        raise Exception('Check the labels assigned in CSV file!')\n",
    "    return labels,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:/RoadCrossingAssistant/Data/Videos\\video1.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video2.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video3.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video4.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video6.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video28.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video66.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video47.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video36.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video19.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video23.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video42.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video18.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video69.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video35.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video102.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video17.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video40.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video85.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video21.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video5.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video51.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video104.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video48.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video49.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video54.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video72.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video77.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video81.MOV\n",
      "shape of labels_train (900,)\n",
      "shape of labels_test (5850,)\n"
     ]
    }
   ],
   "source": [
    "labels_train = []\n",
    "labels_test = []\n",
    "\n",
    "for i in range(len(videos_train[:5])):\n",
    "\n",
    "    print(videos_train[i])\n",
    "    cap = cv2.VideoCapture(videos_train[i])\n",
    "    no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    l, f = get_labels_from_video(no_frames, labels_train_loaded[i])\n",
    "    labels_train.extend(l)\n",
    "\n",
    "for i in range(len(videos_test)):\n",
    "\n",
    "    print(videos_test[i])\n",
    "    cap = cv2.VideoCapture(videos_test[i])\n",
    "    no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    l, f = get_labels_from_video(no_frames, labels_test_loaded[i])\n",
    "    labels_test.extend(l)\n",
    "\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(\"shape of labels_train\", labels_train.shape)\n",
    "print(\"shape of labels_test\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(764,) (136,)\nreducing the number of unsafe frames\n\n\n(3937,) (1913,)\nreducing the number of unsafe frames\n\n\nshape on labels_train_reduced (299,)\nshape of labels_test_reduced (4208,)\n"
     ]
    }
   ],
   "source": [
    "#get required frame indices\n",
    "\n",
    "def get_required_indices(labels):\n",
    "\n",
    "    ind0 = np.where(labels == 0)[0]\n",
    "    ind1 = np.where(labels == 1)[0]\n",
    "\n",
    "    print(ind0.shape, ind1.shape)\n",
    "\n",
    "    if (len(ind0)/len(ind1)) > 1.2:\n",
    "        print('reducing the number of unsafe frames\\n\\n')\n",
    "        len_ind0 = int(len(ind1)*1.2)\n",
    "        ind0 = ind0[:len_ind0]\n",
    "\n",
    "        indices_required = np.concatenate((ind0, ind1))\n",
    "    \n",
    "    return indices_required\n",
    "\n",
    "indices_required_train = get_required_indices(labels_train)\n",
    "indices_required_test = get_required_indices(labels_test)\n",
    "\n",
    "labels_train_reduced = labels_train[indices_required_train]\n",
    "labels_test_reduced = labels_test[indices_required_test]\n",
    "\n",
    "print(\"shape on labels_train_reduced\", labels_train_reduced.shape)\n",
    "print(\"shape of labels_test_reduced\", labels_test_reduced.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:/RoadCrossingAssistant/Data/Videos\\video1.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video2.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video3.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video4.MOV\n",
      "C:/RoadCrossingAssistant/Data/Videos\\video6.MOV\n",
      "(299, 640, 360, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_required_frames(videos, indices_required):\n",
    "\n",
    "    frames = []\n",
    "    ind = -1\n",
    "\n",
    "    for i in range(len(videos)):\n",
    "\n",
    "        print(videos[i])\n",
    "        cap = cv2.VideoCapture(videos[i])\n",
    "        no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        #print(no_frames)\n",
    "        \n",
    "        while cap.isOpened() :\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            ind = ind + 1\n",
    "            if ind in indices_required:\n",
    "                #frames_train.append(frame)\n",
    "                #print(frame.shape)\n",
    "                frame_resized = cv2.resize(frame, (360, 640), interpolation = cv2.INTER_AREA)\n",
    "                #print(frame_resized.shape)\n",
    "                frames.append(frame_resized)\n",
    "    return frames\n",
    "\n",
    "frames_train = get_required_frames(videos_train, indices_required_train)\n",
    "frames_test = get_required_frames(videos_test, indices_required_test)\n",
    "\n",
    "print(\"shape of frames_train: \",frames_train.shape)\n",
    "print(\"shape of frames_test: \",frames_test.shape)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(640.0, 360.0)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "1920/3,1080/3"
   ]
  }
 ]
}